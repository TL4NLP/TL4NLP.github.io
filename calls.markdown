---
layout: page
title: Call For Papers
permalink: /Calls/
nav_order: 2
---

## Topics

We welcome archival and non-archival submissions on theory and applications that relate to transfer learning in NLP including, but not limited to:
- Predicting and quantifying transferability
- Characterizing positive and negative transfer
- Modular Transfer Learning
- Parameter-efficient and Computationally Efficient Transfer
- Domain Adaptation
- Task Transfer
- Multitask, Continual, and Meta Learning
- Cross-lingual Transfer
- Robustness and Generalizability
- Datasets and Tasks for Pre-training and Intermediate fine-tuning. 
- Inductive Transfer Bias in Model Architectures
- Unsupervised/Self-supervised Learning for Transfer (eg. GPT-3)
- Multitask Learning for zero-shot task generalization (eg. T0, FLAN, etc.)



## Formats

We will accept the following paper formats:
* **Long papers** of original, unpublished research. Long papers must not exceed 8 pages, excluding references and appendices. Final versions will be given an additional page of content. Submissions must be anonymized.
* **Short papers** of original, unpublished research. Short papers must not exceed 4 pages, excluding references and appendices. Final versions will be given an additional page of content. Submissions must be anonymized.
* **Non-archival extended abstracts** with either previously-published content or content related to in-progress, unfinished work. Extended abstracts must be limited to 2 pages or fewer, excluding references and appendices. Anonymization is not required.

## Submission Guidelines

**TODO:** Update guidelines according to NeurIPS 

## Dates

* **Sep 22nd:** OpenReview submission deadline
* **Oct 20th:** Acceptance notifications
