---
layout: page
title: Program
permalink: /Program/
nav_order: 3
---

# Schedule

8:50 AM - 5:00 PM (Central Time), Dec 3rd, 2022

| Event | | Time |
| :--- | --- | :--- |
| Opening Remarks | Organizing Team | 8:50-9:00am |
| Invited Talk | Jonas Pfeiffer | 9:00-9:45am |
| Invited Talk | Graham Neubig | 9:45-10:30am |
| Invited Talk | Percy Liang & Ananya Kumar | 10:30-11:15am |
| Break | - | 11:15-11:30am |
| Debate | Sara Hooker & Kyunghyun Cho | 11:30am-12:30pm |
| Lunch | - | 12:30-2:00pm |
| Invited Talk | David Adelani | 2:00-2:45pm |
| Invited Talk | Mike Lewis | 2:45-3:30pm |
| Poster Session | - | 3:30-5:00pm |

# Talk Details

## Jonas Pfeiffer

TBD

## Graham Neubig

TBD

## Percy Liang & Ananya Kumar

**Title:** Fine-Tuning without Distortion: Improving Robustness to Distribution Shifts

**Abstract:** Fine-tuning foundation models (such as BERT or CLIP) is one of the most successful ways to achieve high accuracy. But achieving high in-distribution accuracy is not enough: high-stakes applications such as self-driving cars, medical diagnosis, and poverty mapping, also require models that generalize to circumstances not seen in the fine-tuning distribution. To examine this, we also evaluate models on out-of-distribution (OOD) test data. We show that standard full fine-tuning of all the model’s parameters can distort pretrained information and underperform OOD. Instead, we explain why selectively tuning parts of the model (e.g., prefixes, linear probes, embedding layers) can preserve pretrained information and lead to better OOD performance. Our analysis suggests the easy two-step strategy of linear probing then full fine-tuning (LP-FT), which improves pretrained features without distortion, and leads to even higher accuracies. These works underscore the importance of preserving pretrained knowledge when using powerful pretrained models.

**Percy:** Percy Liang is an Associate Professor of Computer Science at Stanford University (B.S. from MIT, 2004; Ph.D. from UC Berkeley, 2011) and the director of the Center for Research on Foundation Models.  His research spans many topics in machine learning and natural language processing, including robustness, interpretability, semantics, and reasoning.  He is also a strong proponent of reproducibility through the creation of CodaLab Worksheets.  His awards include the Presidential Early Career Award for Scientists and Engineers (2019), IJCAI Computers and Thought Award (2016), an NSF CAREER Award (2016), a Sloan Research Fellowship (2015), a Microsoft Research Faculty Fellowship (2014), and multiple paper awards at ACL, EMNLP, ICML, and COLT.

**Ananya:** Ananya Kumar is a PhD student at Stanford University. His research focuses on understanding and developing better algorithms for pretraining and fine-tuning, and building ML models that are robust to distribution shifts. He has done some of the first work on theoretically analyzing fine-tuning and self-supervised learning. His theories and methods have led to practical methods and state-of-the-art accuracies on many datasets including ImageNet and WILDS. His work has been recognized by an SGF Fellowship, and oral and spotlight presentations at ICLR, ICML, and NeurIPS.

## David Adelani

**Title:** Cross-lingual Transfer for Named Entity Recognition: A study on African Languages

**Abstract:** Multilingual pre-trained language models (PLMs) have demonstrated impressive performance on several downstream tasks for both high-resourced and low-resource languages. However, there is still a large performance drop for languages unseen during pre-training, especially African languages. Similarly, in limited labelled data scenario, cross-lingual transfer learning with PLMs provides an opportunity for fast adaptation to new languages in both zero- and few-shot scenarios. In this talk, we will discuss five components of effective cross-lingual transfer for named entity recognition (NER) task including (1) availability of typologically diverse multilingual benchmark datasets for transfer (2) development of highly effective and easy-to-adapt multilingual PLMs (3) building effective and parameter-efficient cross-lingual transfer frameworks (4) making use of the same domain for both source and target transfer languages (5) choosing the best source transfer language for adaptation. Our evaluation on MasakhaNER -- a benchmark dataset for 21 African languages shows that each of these components significantly improves transfer results. 

**Bio:** David Ifeoluwa Adelani is a research fellow at University College London, United Kingdom. and an active member of Masakhane NLP - a grassroots organization whose mission is to strengthen and spur NLP research in African languages, for Africans, by Africans. Previously, he was a PhD student in computer science at Saarland University, Saarbrücken, Germany. His current research focuses on NLP for under-resourced languages with a focus on African languages, multilingual representation learning, and privacy in NLP.

## Mike Lewis

TBD
